from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

print("ğŸš€ Spark ì´ˆê¸°í™” ì¤‘...")

spark = SparkSession.builder \
    .appName("EcommerceToDatabase") \
    .config("spark.jars.packages", 
            "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,"
            "org.postgresql:postgresql:42.6.0") \
    .config("spark.sql.session.timeZone", "Asia/Seoul") \
    .master("local[*]") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

print("âœ… Spark ì¤€ë¹„ ì™„ë£Œ! (íƒ€ì„ì¡´: Asia/Seoul)")

# PostgreSQL ì—°ê²° ì •ë³´
jdbc_url = "jdbc:postgresql://localhost:5432/analytics_db"
db_properties = {
    "user": "admin",
    "password": "admin123",
    "driver": "org.postgresql.Driver"
}

# Kafka ìŠ¤íŠ¸ë¦¼ ì½ê¸°
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "orders") \
    .option("startingOffsets", "latest") \
    .load()

schema = StructType([
    StructField("order_id", StringType()),
    StructField("timestamp", StringType()),
    StructField("user_id", StringType()),
    StructField("product_id", StringType()),
    StructField("product_name", StringType()),
    StructField("price", IntegerType()),
    StructField("quantity", IntegerType()),
    StructField("ip_address", StringType()),
    StructField("payment_method", StringType())
])

orders = df.select(
    from_json(col("value").cast("string"), schema).alias("data")
).select("data.*") \
 .withColumn("timestamp", to_timestamp(col("timestamp"))) \
 .withColumn("total_price", col("price") * col("quantity"))

print("\n" + "=" * 80)
print("ğŸ’¾ PostgreSQL ì €ì¥ ì‹œì‘! (KST ì‹œê°„)")
print("=" * 80)

# ==========================================
# 1. ì¸ê¸° ìƒí’ˆ â†’ DB ì €ì¥
# ==========================================
popular_products = orders \
    .withWatermark("timestamp", "30 seconds") \
    .groupBy(
        window(col("timestamp"), "30 seconds"),
        col("product_name")
    ) \
    .agg(
        count("*").alias("order_count"),
        sum("total_price").alias("total_sales")
    ) \
    .select(
        col("window.start").alias("window_start"),
        col("window.end").alias("window_end"),
        col("product_name"),
        col("order_count"),
        col("total_sales")
    )

def save_popular_products(batch_df, batch_id):
    if batch_df.count() > 0:
        batch_df.write \
            .jdbc(url=jdbc_url, table="popular_products", 
                  mode="append", properties=db_properties)
        print(f"âœ… [Batch {batch_id}] ì¸ê¸° ìƒí’ˆ {batch_df.count()}ê±´ ì €ì¥")

query1 = popular_products \
    .writeStream \
    .foreachBatch(save_popular_products) \
    .outputMode("update") \
    .trigger(processingTime='30 seconds') \
    .start()

# ==========================================
# 2. ì˜ì‹¬ IP â†’ DB ì €ì¥
# ==========================================
suspicious_ips = orders \
    .withWatermark("timestamp", "1 minute") \
    .groupBy(
        window(col("timestamp"), "1 minute"),
        col("ip_address")
    ) \
    .agg(
        count("*").alias("order_count"),
        sum("total_price").alias("total_spent")
    ) \
    .filter(col("order_count") >= 10) \
    .select(
        col("window.start").alias("window_start"),
        col("ip_address"),
        col("order_count"),
        col("total_spent")
    )

def save_suspicious_ips(batch_df, batch_id):
    if batch_df.count() > 0:
        batch_df.write \
            .jdbc(url=jdbc_url, table="suspicious_ips", 
                  mode="append", properties=db_properties)
        print(f"ğŸš¨ [Batch {batch_id}] ì˜ì‹¬ IP {batch_df.count()}ê±´ ì €ì¥")

query2 = suspicious_ips \
    .writeStream \
    .foreachBatch(save_suspicious_ips) \
    .outputMode("update") \
    .trigger(processingTime='30 seconds') \
    .start()

# ==========================================
# 3. ê³ ì•¡ ê±°ë˜ â†’ DB ì €ì¥
# ==========================================
high_value = orders \
    .filter(col("total_price") >= 500000) \
    .select(
        col("timestamp").alias("order_time"),
        col("order_id"),
        col("product_name"),
        col("quantity"),
        col("total_price"),
        col("ip_address")
    )

def save_high_value(batch_df, batch_id):
    if batch_df.count() > 0:
        batch_df.write \
            .jdbc(url=jdbc_url, table="high_value_orders", 
                  mode="append", properties=db_properties)
        print(f"ğŸ’° [Batch {batch_id}] ê³ ì•¡ ê±°ë˜ {batch_df.count()}ê±´ ì €ì¥")

query3 = high_value \
    .writeStream \
    .foreachBatch(save_high_value) \
    .outputMode("append") \
    .start()

# ==========================================
# 4. ì‹¤ì‹œê°„ í†µê³„ â†’ DB ì €ì¥
# ==========================================
stats = orders \
    .withWatermark("timestamp", "30 seconds") \
    .groupBy(window(col("timestamp"), "30 seconds")) \
    .agg(
        count("*").alias("total_orders"),
        sum("total_price").alias("total_sales"),
        avg("total_price").alias("avg_order_value")
    ) \
    .select(
        col("window.end").alias("stat_time"),
        col("total_orders"),
        col("total_sales"),
        col("avg_order_value").cast("bigint")
    ) \
    .withColumn("suspicious_ip_count", lit(0))

def save_stats(batch_df, batch_id):
    if batch_df.count() > 0:
        batch_df.write \
            .jdbc(url=jdbc_url, table="realtime_stats", 
                  mode="append", properties=db_properties)
        print(f"ğŸ“Š [Batch {batch_id}] ì‹¤ì‹œê°„ í†µê³„ ì €ì¥")

query4 = stats \
    .writeStream \
    .foreachBatch(save_stats) \
    .outputMode("update") \
    .trigger(processingTime='30 seconds') \
    .start()

print("\nğŸ“Š ì €ì¥ ëŒ€ì‹œë³´ë“œ:")
print("  [1] ì¸ê¸° ìƒí’ˆ - 30ì´ˆë§ˆë‹¤ (KST)")
print("  [2] ì˜ì‹¬ IP - 30ì´ˆë§ˆë‹¤ (KST)")
print("  [3] ê³ ì•¡ ê±°ë˜ - ì‹¤ì‹œê°„ (KST)")
print("  [4] í†µê³„ - 30ì´ˆë§ˆë‹¤ (KST)")
print("\nğŸ’¡ Ctrl+Cë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•˜ì„¸ìš”\n")

try:
    query1.awaitTermination()
except KeyboardInterrupt:
    print("\nğŸ›‘ ì €ì¥ ì¤‘ë‹¨")
    spark.stop()